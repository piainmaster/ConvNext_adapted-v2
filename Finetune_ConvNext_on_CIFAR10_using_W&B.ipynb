{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piainmaster/ConvNext_adapted-v2/blob/pn%2FResNeXt-ify_depthconv/Finetune_ConvNext_on_CIFAR10_using_W%26B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this notebook to finetune a ConvNeXt-tiny model on CIFAR 10 dataset. The [official ConvNeXt repository](https://github.com/facebookresearch/ConvNeXt) is instrumented with [Weights and Biases](https://wandb.ai/site). You can now easily log your train/test metrics and version control your model checkpoints to Weigths and Biases"
      ],
      "metadata": {
        "id": "LniKjqdogsrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öΩÔ∏è Installation and Setup\n",
        "\n",
        "The following installation instruction is based on [INSTALL.md](https://github.com/facebookresearch/ConvNeXt/blob/main/INSTALL.md) provided by the official ConvNeXt repository."
      ],
      "metadata": {
        "id": "1JS4ffXFRnRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -qq wandb timm==0.4.12 six tensorboardX"
      ],
      "metadata": {
        "id": "5YbEGpKrDKC5",
        "outputId": "5a797398-c5dc-440c-cf21-7f883266f004",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the official ConvNeXt respository."
      ],
      "metadata": {
        "id": "kDXQ-EpX9fsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b main https://github.com/piainmaster/ConvNext_adapted-v2/ ConvNeXt_finetune\n",
        "!git clone -b pn/ResNeXt-ify_depthconv https://github.com/piainmaster/ConvNext_adapted-v2/ ConvNeXt_revertResNeXt-ify_depthconv\n"
      ],
      "metadata": {
        "id": "zmmHO1Cp4E90",
        "outputId": "0b83ab25-30d8-41df-a839-f18950b1b9a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ConvNeXt_finetune'...\n",
            "remote: Enumerating objects: 359, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 359 (delta 199), reused 343 (delta 191), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (359/359), 93.50 KiB | 5.50 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n",
            "Cloning into 'ConvNeXt_revertResNeXt-ify_depthconv'...\n",
            "remote: Enumerating objects: 359, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 359 (delta 199), reused 343 (delta 191), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (359/359), 93.50 KiB | 5.50 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèÄ Download the Dataset\n",
        "\n",
        "We will be finetuning on CIFAR-10 dataset. To use any custom dataset (CIFAR-10 here) the format of the dataset should be as shown below:\n",
        "\n",
        "```\n",
        "/path/to/dataset/\n",
        "  train/\n",
        "    class1/\n",
        "      img1.jpeg\n",
        "    class2/\n",
        "      img2.jpeg\n",
        "  val/\n",
        "    class1/\n",
        "      img3.jpeg\n",
        "    class2/\n",
        "      img4.jpeg\n",
        "```\n",
        "\n",
        "I have used this [repository](https://github.com/YoongiKim/CIFAR-10-images) that has the CIFAR-10 images in the required format."
      ],
      "metadata": {
        "id": "yoVwkQ0v80KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YoongiKim/CIFAR-10-images"
      ],
      "metadata": {
        "id": "8xcQ6QV41k8S",
        "outputId": "faee5e54-58c2-41b1-8ab4-0526acf0922b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CIFAR-10-images'...\n",
            "remote: Enumerating objects: 60027, done.\u001b[K\n",
            "remote: Total 60027 (delta 0), reused 0 (delta 0), pack-reused 60027\u001b[K\n",
            "Receiving objects: 100% (60027/60027), 19.94 MiB | 37.06 MiB/s, done.\n",
            "Resolving deltas: 100% (59990/59990), done.\n",
            "Updating files: 100% (60001/60001), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèà Download Pretrained Weights\n",
        "\n",
        "We will be finetuning the ConvNeXt Tiny model pretrained on ImageNet 1K dataset."
      ],
      "metadata": {
        "id": "J6qUVfL29tH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ConvNeXt_revertResNeXt-ify_depthconv/\n",
        "!wget https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth"
      ],
      "metadata": {
        "id": "TYPDl5bT8LZ5",
        "outputId": "29085e39-86fe-432b-8274-0f056f9f8f54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ConvNeXt_revertResNeXt-ify_depthconv\n",
            "--2024-02-22 10:00:06--  https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.224.14.65, 13.224.14.69, 13.224.14.100, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.224.14.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 114414741 (109M) [binary/octet-stream]\n",
            "Saving to: ‚Äòconvnext_tiny_1k_224_ema.pth‚Äô\n",
            "\n",
            "convnext_tiny_1k_22 100%[===================>] 109.11M   181MB/s    in 0.6s    \n",
            "\n",
            "2024-02-22 10:00:07 (181 MB/s) - ‚Äòconvnext_tiny_1k_224_ema.pth‚Äô saved [114414741/114414741]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "aY6SgUWtP2nk",
        "outputId": "681bd92d-d764-4f16-8c1b-638a9453b90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéæ Train with Weights and Biases\n",
        "\n",
        "If you want to log the train and evaluation metrics using Weights and Biases pass `--enable_wandb true`.\n",
        "\n",
        "You can also save the finetuned checkpoints as version controlled W&B [Artifacts](https://docs.wandb.ai/guides/artifacts) if you pass `--wandb_ckpt true`.\n",
        "\n"
      ],
      "metadata": {
        "id": "pSPgPCjp-Lro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --epochs 100 \\\n",
        "                --batch_size 64 \\\n",
        "                --model convnext_tiny \\\n",
        "                --data_set image_folder \\\n",
        "                --data_path ../CIFAR-10-images/train \\\n",
        "                --eval_data_path ../CIFAR-10-images/test \\\n",
        "                --nb_classes 10 \\\n",
        "                --num_workers 8 \\\n",
        "                --warmup_epochs 0 \\\n",
        "                --save_ckpt true \\\n",
        "                --output_dir model_ckpt \\\n",
        "                --cutmix 0 \\\n",
        "                --mixup 0 --lr 4e-4 \\\n",
        "                --enable_wandb true --wandb_ckpt true"
      ],
      "metadata": {
        "id": "_8sNl2Mb6x8_",
        "outputId": "436b4269-8c18-469d-bc1d-130c4fcd5bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(batch_size=64, epochs=100, update_freq=1, model='convnext_tiny', drop_path=0, input_size=224, layer_scale_init_value=1e-06, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_ema_eval=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0004, layer_decay=1.0, min_lr=1e-06, warmup_epochs=0, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', head_init_scale=1.0, model_key='model|module', model_prefix='', data_path='../CIFAR-10-images/train', eval_data_path='../CIFAR-10-images/test', nb_classes=10, imagenet_default_mean_and_std=True, data_set='image_folder', output_dir='model_ckpt', log_dir=None, device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, save_ckpt_freq=1, save_ckpt_num=3, start_epoch=0, eval=False, dist_eval=True, disable_eval=False, num_workers=8, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', use_amp=False, enable_wandb=True, project='convnext', wandb_ckpt=True, distributed=False)\n",
            "Transform = \n",
            "RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)\n",
            "RandomHorizontalFlip(p=0.5)\n",
            "<timm.data.auto_augment.RandAugment object at 0x7a3f267be3e0>\n",
            "ToTensor()\n",
            "Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
            "<timm.data.random_erasing.RandomErasing object at 0x7a3f267be020>\n",
            "---------------------------\n",
            "Number of the class = 10\n",
            "Transform = \n",
            "Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)\n",
            "CenterCrop(size=(224, 224))\n",
            "ToTensor()\n",
            "Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "---------------------------\n",
            "Number of the class = 10\n",
            "Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7a3f267be800>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/ConvNeXt_revertResNeXt-ify_depthconv/wandb/run-20240222_100047-kxwt8r4f\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mluminous-noodles-106\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/computer__vision/convnext\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/computer__vision/convnext/runs/kxwt8r4f\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Model = ConvNeXt(\n",
            "  (downsample_layers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(4, 4))\n",
            "      (1): LayerNorm()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): LayerNorm()\n",
            "      (1): Conv2d(64, 192, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): LayerNorm()\n",
            "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): LayerNorm()\n",
            "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "  )\n",
            "  (stages): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Block(\n",
            "        (dwconv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=64, out_features=256, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=256, out_features=64, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (dwconv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=64, out_features=256, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=256, out_features=64, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (dwconv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=64, out_features=256, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=256, out_features=64, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Block(\n",
            "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (3): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (4): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (5): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (6): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (7): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (8): Block(\n",
            "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Block(\n",
            "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "        (norm): LayerNorm()\n",
            "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
            ")\n",
            "number of params: 27673098\n",
            "LR = 0.00040000\n",
            "Batch size = 64\n",
            "Update frequent = 1\n",
            "Number of training examples = 5000\n",
            "Number of training training per epoch = 78\n",
            "Param groups = {\n",
            "  \"decay\": {\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"params\": [\n",
            "      \"downsample_layers.0.0.weight\",\n",
            "      \"downsample_layers.1.1.weight\",\n",
            "      \"downsample_layers.2.1.weight\",\n",
            "      \"downsample_layers.3.1.weight\",\n",
            "      \"stages.0.0.dwconv.weight\",\n",
            "      \"stages.0.0.pwconv1.weight\",\n",
            "      \"stages.0.0.pwconv2.weight\",\n",
            "      \"stages.0.1.dwconv.weight\",\n",
            "      \"stages.0.1.pwconv1.weight\",\n",
            "      \"stages.0.1.pwconv2.weight\",\n",
            "      \"stages.0.2.dwconv.weight\",\n",
            "      \"stages.0.2.pwconv1.weight\",\n",
            "      \"stages.0.2.pwconv2.weight\",\n",
            "      \"stages.1.0.dwconv.weight\",\n",
            "      \"stages.1.0.pwconv1.weight\",\n",
            "      \"stages.1.0.pwconv2.weight\",\n",
            "      \"stages.1.1.dwconv.weight\",\n",
            "      \"stages.1.1.pwconv1.weight\",\n",
            "      \"stages.1.1.pwconv2.weight\",\n",
            "      \"stages.1.2.dwconv.weight\",\n",
            "      \"stages.1.2.pwconv1.weight\",\n",
            "      \"stages.1.2.pwconv2.weight\",\n",
            "      \"stages.2.0.dwconv.weight\",\n",
            "      \"stages.2.0.pwconv1.weight\",\n",
            "      \"stages.2.0.pwconv2.weight\",\n",
            "      \"stages.2.1.dwconv.weight\",\n",
            "      \"stages.2.1.pwconv1.weight\",\n",
            "      \"stages.2.1.pwconv2.weight\",\n",
            "      \"stages.2.2.dwconv.weight\",\n",
            "      \"stages.2.2.pwconv1.weight\",\n",
            "      \"stages.2.2.pwconv2.weight\",\n",
            "      \"stages.2.3.dwconv.weight\",\n",
            "      \"stages.2.3.pwconv1.weight\",\n",
            "      \"stages.2.3.pwconv2.weight\",\n",
            "      \"stages.2.4.dwconv.weight\",\n",
            "      \"stages.2.4.pwconv1.weight\",\n",
            "      \"stages.2.4.pwconv2.weight\",\n",
            "      \"stages.2.5.dwconv.weight\",\n",
            "      \"stages.2.5.pwconv1.weight\",\n",
            "      \"stages.2.5.pwconv2.weight\",\n",
            "      \"stages.2.6.dwconv.weight\",\n",
            "      \"stages.2.6.pwconv1.weight\",\n",
            "      \"stages.2.6.pwconv2.weight\",\n",
            "      \"stages.2.7.dwconv.weight\",\n",
            "      \"stages.2.7.pwconv1.weight\",\n",
            "      \"stages.2.7.pwconv2.weight\",\n",
            "      \"stages.2.8.dwconv.weight\",\n",
            "      \"stages.2.8.pwconv1.weight\",\n",
            "      \"stages.2.8.pwconv2.weight\",\n",
            "      \"stages.3.0.dwconv.weight\",\n",
            "      \"stages.3.0.pwconv1.weight\",\n",
            "      \"stages.3.0.pwconv2.weight\",\n",
            "      \"stages.3.1.dwconv.weight\",\n",
            "      \"stages.3.1.pwconv1.weight\",\n",
            "      \"stages.3.1.pwconv2.weight\",\n",
            "      \"stages.3.2.dwconv.weight\",\n",
            "      \"stages.3.2.pwconv1.weight\",\n",
            "      \"stages.3.2.pwconv2.weight\",\n",
            "      \"head.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  },\n",
            "  \"no_decay\": {\n",
            "    \"weight_decay\": 0.0,\n",
            "    \"params\": [\n",
            "      \"downsample_layers.0.0.bias\",\n",
            "      \"downsample_layers.0.1.weight\",\n",
            "      \"downsample_layers.0.1.bias\",\n",
            "      \"downsample_layers.1.0.weight\",\n",
            "      \"downsample_layers.1.0.bias\",\n",
            "      \"downsample_layers.1.1.bias\",\n",
            "      \"downsample_layers.2.0.weight\",\n",
            "      \"downsample_layers.2.0.bias\",\n",
            "      \"downsample_layers.2.1.bias\",\n",
            "      \"downsample_layers.3.0.weight\",\n",
            "      \"downsample_layers.3.0.bias\",\n",
            "      \"downsample_layers.3.1.bias\",\n",
            "      \"stages.0.0.gamma\",\n",
            "      \"stages.0.0.dwconv.bias\",\n",
            "      \"stages.0.0.norm.weight\",\n",
            "      \"stages.0.0.norm.bias\",\n",
            "      \"stages.0.0.pwconv1.bias\",\n",
            "      \"stages.0.0.pwconv2.bias\",\n",
            "      \"stages.0.1.gamma\",\n",
            "      \"stages.0.1.dwconv.bias\",\n",
            "      \"stages.0.1.norm.weight\",\n",
            "      \"stages.0.1.norm.bias\",\n",
            "      \"stages.0.1.pwconv1.bias\",\n",
            "      \"stages.0.1.pwconv2.bias\",\n",
            "      \"stages.0.2.gamma\",\n",
            "      \"stages.0.2.dwconv.bias\",\n",
            "      \"stages.0.2.norm.weight\",\n",
            "      \"stages.0.2.norm.bias\",\n",
            "      \"stages.0.2.pwconv1.bias\",\n",
            "      \"stages.0.2.pwconv2.bias\",\n",
            "      \"stages.1.0.gamma\",\n",
            "      \"stages.1.0.dwconv.bias\",\n",
            "      \"stages.1.0.norm.weight\",\n",
            "      \"stages.1.0.norm.bias\",\n",
            "      \"stages.1.0.pwconv1.bias\",\n",
            "      \"stages.1.0.pwconv2.bias\",\n",
            "      \"stages.1.1.gamma\",\n",
            "      \"stages.1.1.dwconv.bias\",\n",
            "      \"stages.1.1.norm.weight\",\n",
            "      \"stages.1.1.norm.bias\",\n",
            "      \"stages.1.1.pwconv1.bias\",\n",
            "      \"stages.1.1.pwconv2.bias\",\n",
            "      \"stages.1.2.gamma\",\n",
            "      \"stages.1.2.dwconv.bias\",\n",
            "      \"stages.1.2.norm.weight\",\n",
            "      \"stages.1.2.norm.bias\",\n",
            "      \"stages.1.2.pwconv1.bias\",\n",
            "      \"stages.1.2.pwconv2.bias\",\n",
            "      \"stages.2.0.gamma\",\n",
            "      \"stages.2.0.dwconv.bias\",\n",
            "      \"stages.2.0.norm.weight\",\n",
            "      \"stages.2.0.norm.bias\",\n",
            "      \"stages.2.0.pwconv1.bias\",\n",
            "      \"stages.2.0.pwconv2.bias\",\n",
            "      \"stages.2.1.gamma\",\n",
            "      \"stages.2.1.dwconv.bias\",\n",
            "      \"stages.2.1.norm.weight\",\n",
            "      \"stages.2.1.norm.bias\",\n",
            "      \"stages.2.1.pwconv1.bias\",\n",
            "      \"stages.2.1.pwconv2.bias\",\n",
            "      \"stages.2.2.gamma\",\n",
            "      \"stages.2.2.dwconv.bias\",\n",
            "      \"stages.2.2.norm.weight\",\n",
            "      \"stages.2.2.norm.bias\",\n",
            "      \"stages.2.2.pwconv1.bias\",\n",
            "      \"stages.2.2.pwconv2.bias\",\n",
            "      \"stages.2.3.gamma\",\n",
            "      \"stages.2.3.dwconv.bias\",\n",
            "      \"stages.2.3.norm.weight\",\n",
            "      \"stages.2.3.norm.bias\",\n",
            "      \"stages.2.3.pwconv1.bias\",\n",
            "      \"stages.2.3.pwconv2.bias\",\n",
            "      \"stages.2.4.gamma\",\n",
            "      \"stages.2.4.dwconv.bias\",\n",
            "      \"stages.2.4.norm.weight\",\n",
            "      \"stages.2.4.norm.bias\",\n",
            "      \"stages.2.4.pwconv1.bias\",\n",
            "      \"stages.2.4.pwconv2.bias\",\n",
            "      \"stages.2.5.gamma\",\n",
            "      \"stages.2.5.dwconv.bias\",\n",
            "      \"stages.2.5.norm.weight\",\n",
            "      \"stages.2.5.norm.bias\",\n",
            "      \"stages.2.5.pwconv1.bias\",\n",
            "      \"stages.2.5.pwconv2.bias\",\n",
            "      \"stages.2.6.gamma\",\n",
            "      \"stages.2.6.dwconv.bias\",\n",
            "      \"stages.2.6.norm.weight\",\n",
            "      \"stages.2.6.norm.bias\",\n",
            "      \"stages.2.6.pwconv1.bias\",\n",
            "      \"stages.2.6.pwconv2.bias\",\n",
            "      \"stages.2.7.gamma\",\n",
            "      \"stages.2.7.dwconv.bias\",\n",
            "      \"stages.2.7.norm.weight\",\n",
            "      \"stages.2.7.norm.bias\",\n",
            "      \"stages.2.7.pwconv1.bias\",\n",
            "      \"stages.2.7.pwconv2.bias\",\n",
            "      \"stages.2.8.gamma\",\n",
            "      \"stages.2.8.dwconv.bias\",\n",
            "      \"stages.2.8.norm.weight\",\n",
            "      \"stages.2.8.norm.bias\",\n",
            "      \"stages.2.8.pwconv1.bias\",\n",
            "      \"stages.2.8.pwconv2.bias\",\n",
            "      \"stages.3.0.gamma\",\n",
            "      \"stages.3.0.dwconv.bias\",\n",
            "      \"stages.3.0.norm.weight\",\n",
            "      \"stages.3.0.norm.bias\",\n",
            "      \"stages.3.0.pwconv1.bias\",\n",
            "      \"stages.3.0.pwconv2.bias\",\n",
            "      \"stages.3.1.gamma\",\n",
            "      \"stages.3.1.dwconv.bias\",\n",
            "      \"stages.3.1.norm.weight\",\n",
            "      \"stages.3.1.norm.bias\",\n",
            "      \"stages.3.1.pwconv1.bias\",\n",
            "      \"stages.3.1.pwconv2.bias\",\n",
            "      \"stages.3.2.gamma\",\n",
            "      \"stages.3.2.dwconv.bias\",\n",
            "      \"stages.3.2.norm.weight\",\n",
            "      \"stages.3.2.norm.bias\",\n",
            "      \"stages.3.2.pwconv1.bias\",\n",
            "      \"stages.3.2.pwconv2.bias\",\n",
            "      \"norm.weight\",\n",
            "      \"norm.bias\",\n",
            "      \"head.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  }\n",
            "}\n",
            "Use Cosine LR scheduler\n",
            "Set warmup steps = 0\n",
            "Set warmup steps = 0\n",
            "Max WD = 0.0500000, Min WD = 0.0500000\n",
            "criterion = LabelSmoothingCrossEntropy()\n",
            "Auto resume checkpoint: \n",
            "Start training for 100 epochs\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "Epoch: [0]  [ 0/78]  eta: 0:09:54  lr: 0.000400  min_lr: 0.000400  loss: 2.3432 (2.3432)  class_acc: 0.1719 (0.1719)  weight_decay: 0.0500 (0.0500)  time: 7.6202  data: 4.3563  max mem: 7432\n",
            "Epoch: [0]  [10/78]  eta: 0:01:29  lr: 0.000400  min_lr: 0.000400  loss: 2.5827 (2.8038)  class_acc: 0.1250 (0.1278)  weight_decay: 0.0500 (0.0500)  time: 1.3188  data: 0.3970  max mem: 7432\n",
            "Epoch: [0]  [20/78]  eta: 0:00:59  lr: 0.000400  min_lr: 0.000400  loss: 2.5082 (2.6353)  class_acc: 0.1250 (0.1272)  weight_decay: 0.0500 (0.0500)  time: 0.6911  data: 0.0011  max mem: 7432\n",
            "Epoch: [0]  [30/78]  eta: 0:00:44  lr: 0.000400  min_lr: 0.000400  loss: 2.3577 (2.5356)  class_acc: 0.1250 (0.1250)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.0014  max mem: 7432\n",
            "Epoch: [0]  [40/78]  eta: 0:00:32  lr: 0.000400  min_lr: 0.000400  loss: 2.3071 (2.4804)  class_acc: 0.1250 (0.1288)  weight_decay: 0.0500 (0.0500)  time: 0.7006  data: 0.0013  max mem: 7432\n",
            "Epoch: [0]  [50/78]  eta: 0:00:23  lr: 0.000400  min_lr: 0.000400  loss: 2.2888 (2.4432)  class_acc: 0.1406 (0.1354)  weight_decay: 0.0500 (0.0500)  time: 0.7080  data: 0.0009  max mem: 7432\n",
            "Epoch: [0]  [60/78]  eta: 0:00:14  lr: 0.000400  min_lr: 0.000400  loss: 2.2888 (2.4187)  class_acc: 0.1562 (0.1399)  weight_decay: 0.0500 (0.0500)  time: 0.7137  data: 0.0009  max mem: 7432\n",
            "Epoch: [0]  [70/78]  eta: 0:00:06  lr: 0.000400  min_lr: 0.000400  loss: 2.3049 (2.4058)  class_acc: 0.1406 (0.1360)  weight_decay: 0.0500 (0.0500)  time: 0.7159  data: 0.0007  max mem: 7432\n",
            "Epoch: [0]  [77/78]  eta: 0:00:00  lr: 0.000400  min_lr: 0.000400  loss: 2.2921 (2.3933)  class_acc: 0.1406 (0.1366)  weight_decay: 0.0500 (0.0500)  time: 0.7177  data: 0.0002  max mem: 7432\n",
            "Epoch: [0] Total time: 0:01:02 (0.7985 s / it)\n",
            "Averaged stats: lr: 0.000400  min_lr: 0.000400  loss: 2.2921 (2.3933)  class_acc: 0.1406 (0.1366)  weight_decay: 0.0500 (0.0500)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Test:  [ 0/11]  eta: 0:00:36  loss: 2.1757 (2.1757)  acc1: 17.7083 (17.7083)  acc5: 71.8750 (71.8750)  time: 3.3284  data: 2.5434  max mem: 7432\n",
            "Test:  [10/11]  eta: 0:00:00  loss: 2.1384 (2.1361)  acc1: 20.8333 (21.5000)  acc5: 72.9167 (73.2000)  time: 0.6131  data: 0.2313  max mem: 7432\n",
            "Test: Total time: 0:00:07 (0.6441 s / it)\n",
            "* Acc@1 21.500 Acc@5 73.200 loss 2.136\n",
            "Accuracy of the model on the 1000 test images: 21.5%\n",
            "Max accuracy: 21.50%\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "Epoch: [1]  [ 0/78]  eta: 0:04:53  lr: 0.000400  min_lr: 0.000400  loss: 2.2545 (2.2545)  class_acc: 0.2031 (0.2031)  weight_decay: 0.0500 (0.0500)  time: 3.7595  data: 2.9345  max mem: 7432\n",
            "Epoch: [1]  [10/78]  eta: 0:01:08  lr: 0.000400  min_lr: 0.000400  loss: 2.2887 (2.2864)  class_acc: 0.1406 (0.1420)  weight_decay: 0.0500 (0.0500)  time: 1.0011  data: 0.2685  max mem: 7432\n",
            "Epoch: [1]  [20/78]  eta: 0:00:50  lr: 0.000400  min_lr: 0.000400  loss: 2.2864 (2.2767)  class_acc: 0.1250 (0.1369)  weight_decay: 0.0500 (0.0500)  time: 0.7251  data: 0.0024  max mem: 7432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèê Conclusion\n",
        "\n",
        "* **The above setting gives a top-1 accuracy of ~95%.**\n",
        "* The ConvNeXt repository comes with modern training regimes and is easy to finetune on any dataset.\n",
        "* The finetune model achieves competitive results.\n",
        "\n",
        "* By passing two arguments you get the following:\n",
        "\n",
        "  * Repository of all your experiments (train and test metrics) as a [W&B Project](https://docs.wandb.ai/ref/app/pages/project-page). You can easily compare experiments to find the best performing model.\n",
        "  * Hyperparameters (Configs) used to train individual models.\n",
        "  * System (CPU/GPU/Disk) metrics.\n",
        "  * Model checkpoints saved as W&B Artifacts. They are versioned and easy to share.\n",
        "\n",
        "  Check out the associated [W&B run page](https://wandb.ai/ayut/convnext/runs/16vi9e31). $‚Üí$"
      ],
      "metadata": {
        "id": "350MmZgtBVWy"
      }
    }
  ]
}